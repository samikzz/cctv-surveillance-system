{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9215393,"sourceType":"datasetVersion","datasetId":5572437},{"sourceId":9229130,"sourceType":"datasetVersion","datasetId":5581936}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q pytorchvideo evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install -q pyarrow==14.0.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install -q transformers --upgrade","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install -q torch==2.0.1 torchvision==0.15.2 --extra-index-url https://download.pytorch.org/whl/cu118 xformers==0.0.21","metadata":{"execution":{"iopub.status.busy":"2024-08-26T09:12:10.395429Z","iopub.execute_input":"2024-08-26T09:12:10.395922Z","iopub.status.idle":"2024-08-26T09:15:12.307959Z","shell.execute_reply.started":"2024-08-26T09:12:10.395887Z","shell.execute_reply":"2024-08-26T09:15:12.305132Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchaudio 2.1.2+cpu requires torch==2.1.2, but you have torch 2.0.1+cu118 which is incompatible.\ntorchtext 0.16.2+cpu requires torch==2.1.2, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\n\nroot_path = \"/kaggle/input/my-data\"\nfolder_list = os.listdir(root_path)\nlabel_list = [path for path in folder_list if not path.endswith((\".csv\"))]\ntotal_df = pd.read_csv(os.path.join(root_path,\"train.csv\"))\n\ntotal_df.reset_index(drop = True, inplace = True)\ntotal_df['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:31:10.017181Z","iopub.execute_input":"2024-08-26T06:31:10.017906Z","iopub.status.idle":"2024-08-26T06:31:10.430515Z","shell.execute_reply.started":"2024-08-26T06:31:10.017864Z","shell.execute_reply":"2024-08-26T06:31:10.429679Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"label\nnormal      222\ntheft       156\nfight       101\naccident     97\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndef correct_file_path(file_name: str, root_path: str):\n    return os.path.join(root_path, file_name.replace('\\\\', '/'))\n\ndef preprocess_meta_df(df, root_path, label2id):\n    df.rename(columns={\"video_name\": \"video_path\"}, inplace=True)\n    df['video_path'] = df['video_path'].apply(lambda x: correct_file_path(x, root_path))\n    df['label'] = df['label'].apply(lambda x: label2id[x])\n    return df\n\ntrain_meta_df, test_meta_df = train_test_split(total_df, test_size=0.2, random_state=42)\n\nlabel_list = list(set(train_meta_df['label']))\nclass_labels = sorted(label_list)\nlabel2id = {label: i for i, label in enumerate(class_labels)}\nid2label = {i: label for label, i in label2id.items()}\n\nprint(f\"Unique classes: {list(label2id.keys())}.\")\n\ntrain_meta_df = preprocess_meta_df(train_meta_df, root_path, label2id)\ntest_meta_df = preprocess_meta_df(test_meta_df, root_path, label2id)\n\nprint(\"Splitted data:\", len(train_meta_df), len(test_meta_df))","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:31:10.431813Z","iopub.execute_input":"2024-08-26T06:31:10.432148Z","iopub.status.idle":"2024-08-26T06:31:11.587120Z","shell.execute_reply.started":"2024-08-26T06:31:10.432122Z","shell.execute_reply":"2024-08-26T06:31:11.585668Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Unique classes: ['accident', 'fight', 'normal', 'theft'].\nSplitted data: 460 116\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport pytorchvideo.data\nfrom torch.utils.data import Dataset\nfrom transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification, VivitConfig\nfrom transformers import VivitImageProcessor, VivitModel, VivitForVideoClassification\nimport torch.nn as nn\n\nfrom pytorchvideo.transforms import (\n    ApplyTransformToKey,\n    Normalize,\n    RandomShortSideScale,\n    RemoveKey,\n    ShortSideScale,\n    UniformTemporalSubsample,\n)\n\nfrom torchvision.transforms import (\n    Compose,\n    Lambda,\n    RandomCrop,\n    RandomHorizontalFlip,\n    Resize,\n)\n\nmodel_checkpoint = \"google/vivit-b-16x2-kinetics400\"\n\nmodel1 = VivitForVideoClassification.from_pretrained(model_checkpoint, label2id=label2id, id2label=id2label, ignore_mismatched_sizes=True)\nconf = model1.config\nconf.hidden_size = 492\nconf.num_hidden_layers = 10\nmodel = VivitForVideoClassification.from_pretrained(model_checkpoint, config = conf, ignore_mismatched_sizes=True)\nimage_processor = VivitImageProcessor.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:31:11.588565Z","iopub.execute_input":"2024-08-26T06:31:11.588984Z","iopub.status.idle":"2024-08-26T06:31:31.826923Z","shell.execute_reply.started":"2024-08-26T06:31:11.588950Z","shell.execute_reply":"2024-08-26T06:31:31.826085Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-08-26 06:31:18.034918: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-26 06:31:18.034974: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-26 06:31:18.039913: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of VivitForVideoClassification were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized because the shapes did not match:\n- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([4]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of VivitForVideoClassification were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized because the shapes did not match:\n- vivit.embeddings.cls_token: found shape torch.Size([1, 1, 768]) in the checkpoint and torch.Size([1, 1, 492]) in the model instantiated\n- vivit.embeddings.position_embeddings: found shape torch.Size([1, 3137, 768]) in the checkpoint and torch.Size([1, 3137, 492]) in the model instantiated\n- vivit.embeddings.patch_embeddings.projection.weight: found shape torch.Size([768, 3, 2, 16, 16]) in the checkpoint and torch.Size([492, 3, 2, 16, 16]) in the model instantiated\n- vivit.embeddings.patch_embeddings.projection.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.0.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.0.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.0.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.0.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.0.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.0.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.0.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.0.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.0.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 492]) in the model instantiated\n- vivit.encoder.layer.0.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([492, 3072]) in the model instantiated\n- vivit.encoder.layer.0.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.0.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.0.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.0.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.0.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.1.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.1.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.1.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.1.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.1.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.1.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.1.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.1.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.1.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 492]) in the model instantiated\n- vivit.encoder.layer.1.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([492, 3072]) in the model instantiated\n- vivit.encoder.layer.1.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.1.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.1.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.1.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.1.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.2.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.2.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.2.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.2.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.2.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.2.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.2.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.2.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.2.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 492]) in the model instantiated\n- vivit.encoder.layer.2.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([492, 3072]) in the model instantiated\n- vivit.encoder.layer.2.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.2.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.2.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.2.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.2.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.3.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.3.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.3.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.3.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.3.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.3.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.3.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.3.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.3.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 492]) in the model instantiated\n- vivit.encoder.layer.3.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([492, 3072]) in the model instantiated\n- vivit.encoder.layer.3.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.3.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.3.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.3.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.3.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.4.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.4.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.4.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.4.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.4.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.4.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.4.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.4.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.4.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 492]) in the model instantiated\n- vivit.encoder.layer.4.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([492, 3072]) in the model instantiated\n- vivit.encoder.layer.4.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.4.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.4.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.4.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.4.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.5.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.5.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.5.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.5.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.5.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.5.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.5.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.5.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.5.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 492]) in the model instantiated\n- vivit.encoder.layer.5.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([492, 3072]) in the model instantiated\n- vivit.encoder.layer.5.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.5.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.5.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.5.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.5.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.6.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.6.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.6.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.6.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.6.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.6.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.6.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.6.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.6.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 492]) in the model instantiated\n- vivit.encoder.layer.6.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([492, 3072]) in the model instantiated\n- vivit.encoder.layer.6.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.6.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.6.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.6.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.6.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.7.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.7.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.7.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.7.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.7.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.7.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.7.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.7.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.7.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 492]) in the model instantiated\n- vivit.encoder.layer.7.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([492, 3072]) in the model instantiated\n- vivit.encoder.layer.7.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.7.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.7.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.7.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.7.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.8.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.8.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.8.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.8.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.8.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.8.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.8.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.8.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.8.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 492]) in the model instantiated\n- vivit.encoder.layer.8.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([492, 3072]) in the model instantiated\n- vivit.encoder.layer.8.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.8.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.8.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.8.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.8.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.9.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.9.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.9.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.9.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.9.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.9.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.9.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([492, 492]) in the model instantiated\n- vivit.encoder.layer.9.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.9.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 492]) in the model instantiated\n- vivit.encoder.layer.9.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([492, 3072]) in the model instantiated\n- vivit.encoder.layer.9.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.9.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.9.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.9.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.encoder.layer.9.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.layernorm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- vivit.layernorm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([492]) in the model instantiated\n- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([4, 492]) in the model instantiated\n- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([4]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"class CustomVideoDataset(Dataset):\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        video_path = row['video_path']\n        label = row['label']\n        return video_path, label\n\nmean = image_processor.image_mean\nstd = image_processor.image_std\n\nif \"shortest_edge\" in image_processor.size:\n    height = width = image_processor.size[\"shortest_edge\"]\nelse:\n    height = image_processor.size[\"height\"]\n    width = image_processor.size[\"width\"]\n\nresize_to = (model.config.image_size, model.config.image_size)\n\nnum_frames_to_sample = model.config.num_frames\nclip_duration = 8\n\ntrain_transform = Compose(\n    [\n        ApplyTransformToKey(\n            key=\"video\",\n            transform=Compose(\n                [\n                    UniformTemporalSubsample(num_frames_to_sample),\n                    Lambda(lambda x: x / 255.0),\n                    Normalize(mean, std),\n                    RandomShortSideScale(min_size=256, max_size=320),\n                    Resize(resize_to),\n                    RandomHorizontalFlip(p=0.5),\n                ]\n            ),\n        ),\n    ]\n)\n\nval_transform = Compose(\n    [\n        ApplyTransformToKey(\n            key=\"video\",\n            transform=Compose(\n                [\n                    UniformTemporalSubsample(num_frames_to_sample),\n                    Lambda(lambda x: x / 255.0),\n                    Normalize(mean, std),\n                    Resize(resize_to),\n                ]\n            ),\n        ),\n    ]\n)\n\ntrain_custom_dataset = CustomVideoDataset(train_meta_df)\ntrain_labeled_video_paths = [(video_path, {'label': label}) for video_path, label in train_custom_dataset]\n\ntest_custom_dataset = CustomVideoDataset(test_meta_df)\ntest_labeled_video_paths = [(video_path, {'label': label}) for video_path, label in test_custom_dataset]","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:31:31.830199Z","iopub.execute_input":"2024-08-26T06:31:31.831017Z","iopub.status.idle":"2024-08-26T06:31:31.881617Z","shell.execute_reply.started":"2024-08-26T06:31:31.830985Z","shell.execute_reply":"2024-08-26T06:31:31.880808Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import imageio\nimport numpy as np\nfrom IPython.display import Image\n\ntrain_dataset = pytorchvideo.data.LabeledVideoDataset(\n    labeled_video_paths =train_labeled_video_paths,\n    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n    decode_audio=False,\n    transform=train_transform,\n)\n\ntest_dataset = pytorchvideo.data.LabeledVideoDataset(\n    labeled_video_paths =test_labeled_video_paths,\n    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n    decode_audio=False,\n    transform=val_transform,\n)\n\ndef unnormalize_img(img):\n    img = (img * std) + mean\n    img = (img * 255).astype(\"uint8\")\n    return img.clip(0, 255)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:31:31.882838Z","iopub.execute_input":"2024-08-26T06:31:31.883108Z","iopub.status.idle":"2024-08-26T06:31:31.910225Z","shell.execute_reply.started":"2024-08-26T06:31:31.883086Z","shell.execute_reply":"2024-08-26T06:31:31.909258Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class CustomVideoDataset1(Dataset):\n\n    def __init__(self, data):\n        super().__init__()\n        self.train_dataset = data\n\n    def __len__(self):\n        return self.train_dataset.num_videos\n\n    def __getitem__(self, idx):\n        video=next(iter(self.train_dataset))\n        \n        return {\"video\": video['video'],\n                \"label\": video['label']}","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:31:31.911459Z","iopub.execute_input":"2024-08-26T06:31:31.911773Z","iopub.status.idle":"2024-08-26T06:31:31.917793Z","shell.execute_reply.started":"2024-08-26T06:31:31.911748Z","shell.execute_reply":"2024-08-26T06:31:31.916756Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"custom_train_data = CustomVideoDataset1(train_dataset)\ncustom_test_data = CustomVideoDataset1(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:31:31.918988Z","iopub.execute_input":"2024-08-26T06:31:31.919284Z","iopub.status.idle":"2024-08-26T06:31:31.930347Z","shell.execute_reply.started":"2024-08-26T06:31:31.919261Z","shell.execute_reply":"2024-08-26T06:31:31.929398Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:31:31.931418Z","iopub.execute_input":"2024-08-26T06:31:31.931738Z","iopub.status.idle":"2024-08-26T06:31:31.943985Z","shell.execute_reply.started":"2024-08-26T06:31:31.931710Z","shell.execute_reply":"2024-08-26T06:31:31.943123Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(custom_train_data, batch_size = 1)\ntest_loader = DataLoader(custom_test_data, batch_size = 1)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:31:31.945144Z","iopub.execute_input":"2024-08-26T06:31:31.945594Z","iopub.status.idle":"2024-08-26T06:31:31.957674Z","shell.execute_reply.started":"2024-08-26T06:31:31.945561Z","shell.execute_reply":"2024-08-26T06:31:31.956824Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport torch.optim as optim","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:31:31.958783Z","iopub.execute_input":"2024-08-26T06:31:31.959119Z","iopub.status.idle":"2024-08-26T06:31:31.970208Z","shell.execute_reply.started":"2024-08-26T06:31:31.959087Z","shell.execute_reply":"2024-08-26T06:31:31.969231Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\noptimizer = optim.AdamW(model.parameters(), lr=5e-5)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:31:31.971333Z","iopub.execute_input":"2024-08-26T06:31:31.972231Z","iopub.status.idle":"2024-08-26T06:31:32.276656Z","shell.execute_reply.started":"2024-08-26T06:31:31.972203Z","shell.execute_reply":"2024-08-26T06:31:32.275513Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"epoch = 2","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:31:32.278107Z","iopub.execute_input":"2024-08-26T06:31:32.278469Z","iopub.status.idle":"2024-08-26T06:31:32.283103Z","shell.execute_reply.started":"2024-08-26T06:31:32.278439Z","shell.execute_reply":"2024-08-26T06:31:32.282093Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"final_metrics = []","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:31:32.286487Z","iopub.execute_input":"2024-08-26T06:31:32.286809Z","iopub.status.idle":"2024-08-26T06:31:32.295753Z","shell.execute_reply.started":"2024-08-26T06:31:32.286784Z","shell.execute_reply":"2024-08-26T06:31:32.294708Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# loss_train = []\n# loss_eval = []\n# acc_train = []\n# acc_eval = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\nimport json\n\nfile_path = 'metrics.json'","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:31:32.296923Z","iopub.execute_input":"2024-08-26T06:31:32.297217Z","iopub.status.idle":"2024-08-26T06:31:32.307991Z","shell.execute_reply.started":"2024-08-26T06:31:32.297192Z","shell.execute_reply":"2024-08-26T06:31:32.307024Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"checkpoint = torch.load('/kaggle/input/model3/pytorch/default/1/model-3.pt')\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/metrics/metrics.json', 'r') as json_file:\n    final_metrics = json.load(json_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = '/kaggle/working/metrics.json'\nwith open(file_path, 'r') as json_file:\n    final_metrics = json.load(json_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_metrics","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:31:32.309219Z","iopub.execute_input":"2024-08-26T06:31:32.310031Z","iopub.status.idle":"2024-08-26T06:31:32.320737Z","shell.execute_reply.started":"2024-08-26T06:31:32.309997Z","shell.execute_reply":"2024-08-26T06:31:32.319835Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}]},{"cell_type":"code","source":"for i in range(epoch):\n    metrics = {}\n    model.train()\n    train_losses = 0.0\n    train_sum = 0.0\n    batch_iterator=tqdm(train_loader, desc=f\"Epoch {i+1:02d}\")\n    for sample in batch_iterator:\n        video, label = sample['video'], sample['label']\n        \n        video = video.to(device)\n        label = torch.tensor([label], device=device)\n\n        optimizer.zero_grad() \n        train_logits = model(video.permute(0, 2, 1, 3, 4))['logits']\n        \n        if train_logits.argmax() == label:\n            train_sum +=1\n                \n        train_loss = F.cross_entropy(train_logits, label)\n        train_losses += train_loss.item()\n        \n        \n        train_loss.backward() \n        optimizer.step() \n    \n    avg_train_loss = train_losses / train_dataset.num_videos\n    train_acc = train_sum / train_dataset.num_videos\n#     loss_train.append(float(avg_train_loss))\n#     acc_train.append(float(train_acc))\n    print(f'train loss: {avg_train_loss}')\n    print(f'train acc: {train_acc}')\n              \n    model.eval()\n    eval_losses = 0.0\n    eval_sum = 0.0\n    for sample in test_loader: \n        video, label = sample['video'], sample['label']\n        \n        video = video.to(device)\n        label = torch.tensor([label], device=device)\n        \n        with torch.no_grad():\n            eval_logits = model(video.permute(0, 2, 1, 3, 4))['logits']\n            eval_loss = F.cross_entropy(eval_logits, label).item()\n            eval_losses += eval_loss\n            \n            if eval_logits.argmax() == label:\n                eval_sum +=1\n    with torch.no_grad():        \n        avg_eval_loss = eval_losses / test_dataset.num_videos\n        eval_acc = eval_sum / test_dataset.num_videos\n    #     loss_eval.append(float(avg_eval_loss)) \n    #     acc_eval.append(float(eval_acc))\n        print(f'val loss: {avg_eval_loss}')\n        print(f'val acc: {eval_acc}')\n\n        metrics['acc_train'] = train_acc\n        metrics['loss_train'] = avg_train_loss\n        metrics['acc_eval'] = eval_acc\n        metrics['loss_eval'] = avg_eval_loss\n        final_metrics.append(metrics)\n\n        torch.cuda.empty_cache()\n\n        PATH = f\"model-{i+1}.pt\"\n        torch.save({\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                }, PATH)\n\n        with open(file_path, 'w') as json_file:\n            json.dump(final_metrics, json_file)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:31:32.322042Z","iopub.execute_input":"2024-08-26T06:31:32.322333Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Epoch 01:   0%|          | 0/460 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\nEpoch 01: 100%|██████████| 460/460 [15:24<00:00,  2.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 1.479778262519318\ntrain acc: 0.34130434782608693\nval loss: 1.4058843393777978\nval acc: 0.3793103448275862\n","output_type":"stream"},{"name":"stderr","text":"Epoch 02:  20%|██        | 93/460 [03:29<13:37,  2.23s/it]","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model, 'model3.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.remove(f'/kaggle/working/metrics.json')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.rename('/kaggle/working/model-2.pt', '/kaggle/working/model-1.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}