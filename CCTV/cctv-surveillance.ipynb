{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["pip install -q pytorchvideo evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pip install -q pyarrow==14.0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pip install -q transformers --upgrade"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T09:12:10.395922Z","iopub.status.busy":"2024-08-26T09:12:10.395429Z","iopub.status.idle":"2024-08-26T09:15:12.307959Z","shell.execute_reply":"2024-08-26T09:15:12.305132Z","shell.execute_reply.started":"2024-08-26T09:12:10.395887Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.1.2+cpu requires torch==2.1.2, but you have torch 2.0.1+cu118 which is incompatible.\n","torchtext 0.16.2+cpu requires torch==2.1.2, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install -q torch==2.0.1 torchvision==0.15.2 --extra-index-url https://download.pytorch.org/whl/cu118 xformers==0.0.21"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T06:31:10.017906Z","iopub.status.busy":"2024-08-26T06:31:10.017181Z","iopub.status.idle":"2024-08-26T06:31:10.430515Z","shell.execute_reply":"2024-08-26T06:31:10.429679Z","shell.execute_reply.started":"2024-08-26T06:31:10.017864Z"},"trusted":true},"outputs":[{"data":{"text/plain":["label\n","normal      222\n","theft       156\n","fight       101\n","accident     97\n","Name: count, dtype: int64"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import pandas as pd\n","\n","root_path = r\"C:\\Users\\acer\\OneDrive\\Desktop\\CCTV_Surveillance\\CCTV\"\n","folder_list = os.listdir(root_path)\n","label_list = [path for path in folder_list if not path.endswith((\".csv\"))]\n","total_df = pd.read_csv(os.path.join(root_path,\"train.csv\"))\n","\n","total_df.reset_index(drop = True, inplace = True)\n","total_df['label'].value_counts()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T06:31:10.432148Z","iopub.status.busy":"2024-08-26T06:31:10.431813Z","iopub.status.idle":"2024-08-26T06:31:11.587120Z","shell.execute_reply":"2024-08-26T06:31:11.585668Z","shell.execute_reply.started":"2024-08-26T06:31:10.432122Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique classes: ['accident', 'fight', 'normal', 'theft'].\n","Splitted data: 460 116\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","def correct_file_path(file_name: str, root_path: str):\n","    return os.path.join(root_path, file_name.replace('\\\\', '/'))\n","\n","def preprocess_meta_df(df, root_path, label2id):\n","    df.rename(columns={\"video_name\": \"video_path\"}, inplace=True)\n","    df['video_path'] = df['video_path'].apply(lambda x: correct_file_path(x, root_path))\n","    df['label'] = df['label'].apply(lambda x: label2id[x])\n","    return df\n","\n","train_meta_df, test_meta_df = train_test_split(total_df, test_size=0.2, random_state=42)\n","\n","label_list = list(set(train_meta_df['label']))\n","class_labels = sorted(label_list)\n","label2id = {label: i for i, label in enumerate(class_labels)}\n","id2label = {i: label for label, i in label2id.items()}\n","\n","print(f\"Unique classes: {list(label2id.keys())}.\")\n","\n","train_meta_df = preprocess_meta_df(train_meta_df, root_path, label2id)\n","test_meta_df = preprocess_meta_df(test_meta_df, root_path, label2id)\n","\n","print(\"Splitted data:\", len(train_meta_df), len(test_meta_df))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T06:31:11.588984Z","iopub.status.busy":"2024-08-26T06:31:11.588565Z","iopub.status.idle":"2024-08-26T06:31:31.826923Z","shell.execute_reply":"2024-08-26T06:31:31.826085Z","shell.execute_reply.started":"2024-08-26T06:31:11.588950Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\acer\\OneDrive\\Desktop\\CCTV_Surveillance\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n","Some weights of VivitForVideoClassification were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([4]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["import torch\n","import pytorchvideo.data\n","from torch.utils.data import Dataset\n","from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification, VivitConfig\n","from transformers import VivitImageProcessor, VivitModel, VivitForVideoClassification\n","import torch.nn as nn\n","\n","from pytorchvideo.transforms import (\n","    ApplyTransformToKey,\n","    Normalize,\n","    RandomShortSideScale,\n","    RemoveKey,\n","    ShortSideScale,\n","    UniformTemporalSubsample,\n",")\n","\n","from torchvision.transforms import (\n","    Compose,\n","    Lambda,\n","    RandomCrop,\n","    RandomHorizontalFlip,\n","    Resize,\n",")\n","\n","model_checkpoint = \"google/vivit-b-16x2-kinetics400\"\n","\n","model = VivitForVideoClassification.from_pretrained(model_checkpoint, label2id=label2id, id2label=id2label, ignore_mismatched_sizes=True)\n","image_processor = VivitImageProcessor.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T06:31:31.831017Z","iopub.status.busy":"2024-08-26T06:31:31.830199Z","iopub.status.idle":"2024-08-26T06:31:31.881617Z","shell.execute_reply":"2024-08-26T06:31:31.880808Z","shell.execute_reply.started":"2024-08-26T06:31:31.830985Z"},"trusted":true},"outputs":[],"source":["class CustomVideoDataset(Dataset):\n","    def __init__(self, dataframe):\n","        self.dataframe = dataframe\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        row = self.dataframe.iloc[idx]\n","        video_path = row['video_path']\n","        label = row['label']\n","        return video_path, label\n","\n","mean = image_processor.image_mean\n","std = image_processor.image_std\n","\n","if \"shortest_edge\" in image_processor.size:\n","    height = width = image_processor.size[\"shortest_edge\"]\n","else:\n","    height = image_processor.size[\"height\"]\n","    width = image_processor.size[\"width\"]\n","\n","resize_to = (model.config.image_size, model.config.image_size)\n","\n","num_frames_to_sample = model.config.num_frames\n","clip_duration = 8\n","\n","train_transform = Compose(\n","    [\n","        ApplyTransformToKey(\n","            key=\"video\",\n","            transform=Compose(\n","                [\n","                    UniformTemporalSubsample(num_frames_to_sample),\n","                    Lambda(lambda x: x / 255.0),\n","                    Normalize(mean, std),\n","                    RandomShortSideScale(min_size=256, max_size=320),\n","                    Resize(resize_to),\n","                    RandomHorizontalFlip(p=0.5),\n","                ]\n","            ),\n","        ),\n","    ]\n",")\n","\n","val_transform = Compose(\n","    [\n","        ApplyTransformToKey(\n","            key=\"video\",\n","            transform=Compose(\n","                [\n","                    UniformTemporalSubsample(num_frames_to_sample),\n","                    Lambda(lambda x: x / 255.0),\n","                    Normalize(mean, std),\n","                    Resize(resize_to),\n","                ]\n","            ),\n","        ),\n","    ]\n",")\n","\n","train_custom_dataset = CustomVideoDataset(train_meta_df)\n","train_labeled_video_paths = [(video_path, {'label': label}) for video_path, label in train_custom_dataset]\n","\n","test_custom_dataset = CustomVideoDataset(test_meta_df)\n","test_labeled_video_paths = [(video_path, {'label': label}) for video_path, label in test_custom_dataset]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T06:31:31.883108Z","iopub.status.busy":"2024-08-26T06:31:31.882838Z","iopub.status.idle":"2024-08-26T06:31:31.910225Z","shell.execute_reply":"2024-08-26T06:31:31.909258Z","shell.execute_reply.started":"2024-08-26T06:31:31.883086Z"},"trusted":true},"outputs":[],"source":["import imageio\n","import numpy as np\n","from IPython.display import Image\n","\n","train_dataset = pytorchvideo.data.LabeledVideoDataset(\n","    labeled_video_paths =train_labeled_video_paths,\n","    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n","    decode_audio=False,\n","    transform=train_transform,\n",")\n","\n","test_dataset = pytorchvideo.data.LabeledVideoDataset(\n","    labeled_video_paths =test_labeled_video_paths,\n","    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n","    decode_audio=False,\n","    transform=val_transform,\n",")\n","\n","def unnormalize_img(img):\n","    img = (img * std) + mean\n","    img = (img * 255).astype(\"uint8\")\n","    return img.clip(0, 255)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T06:31:31.911773Z","iopub.status.busy":"2024-08-26T06:31:31.911459Z","iopub.status.idle":"2024-08-26T06:31:31.917793Z","shell.execute_reply":"2024-08-26T06:31:31.916756Z","shell.execute_reply.started":"2024-08-26T06:31:31.911748Z"},"trusted":true},"outputs":[],"source":["class CustomVideoDataset1(Dataset):\n","\n","    def __init__(self, data):\n","        super().__init__()\n","        self.train_dataset = data\n","\n","    def __len__(self):\n","        return self.train_dataset.num_videos\n","\n","    def __getitem__(self, idx):\n","        video=next(iter(self.train_dataset))\n","        \n","        return {\"video\": video['video'],\n","                \"label\": video['label']}"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T06:31:31.919284Z","iopub.status.busy":"2024-08-26T06:31:31.918988Z","iopub.status.idle":"2024-08-26T06:31:31.930347Z","shell.execute_reply":"2024-08-26T06:31:31.929398Z","shell.execute_reply.started":"2024-08-26T06:31:31.919261Z"},"trusted":true},"outputs":[],"source":["custom_train_data = CustomVideoDataset1(train_dataset)\n","custom_test_data = CustomVideoDataset1(test_dataset)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T06:31:31.931738Z","iopub.status.busy":"2024-08-26T06:31:31.931418Z","iopub.status.idle":"2024-08-26T06:31:31.943985Z","shell.execute_reply":"2024-08-26T06:31:31.943123Z","shell.execute_reply.started":"2024-08-26T06:31:31.931710Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T06:31:31.945594Z","iopub.status.busy":"2024-08-26T06:31:31.945144Z","iopub.status.idle":"2024-08-26T06:31:31.957674Z","shell.execute_reply":"2024-08-26T06:31:31.956824Z","shell.execute_reply.started":"2024-08-26T06:31:31.945561Z"},"trusted":true},"outputs":[],"source":["train_loader = DataLoader(custom_train_data, batch_size = 1)\n","test_loader = DataLoader(custom_test_data, batch_size = 1)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T06:31:31.959119Z","iopub.status.busy":"2024-08-26T06:31:31.958783Z","iopub.status.idle":"2024-08-26T06:31:31.970208Z","shell.execute_reply":"2024-08-26T06:31:31.969231Z","shell.execute_reply.started":"2024-08-26T06:31:31.959087Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import torch.optim as optim"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T06:31:31.972231Z","iopub.status.busy":"2024-08-26T06:31:31.971333Z","iopub.status.idle":"2024-08-26T06:31:32.276656Z","shell.execute_reply":"2024-08-26T06:31:32.275513Z","shell.execute_reply.started":"2024-08-26T06:31:31.972203Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","optimizer = optim.AdamW(model.parameters(), lr=5e-5)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T06:31:32.278469Z","iopub.status.busy":"2024-08-26T06:31:32.278107Z","iopub.status.idle":"2024-08-26T06:31:32.283103Z","shell.execute_reply":"2024-08-26T06:31:32.282093Z","shell.execute_reply.started":"2024-08-26T06:31:32.278439Z"},"trusted":true},"outputs":[],"source":["epoch = 2"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T06:31:32.286809Z","iopub.status.busy":"2024-08-26T06:31:32.286487Z","iopub.status.idle":"2024-08-26T06:31:32.295753Z","shell.execute_reply":"2024-08-26T06:31:32.294708Z","shell.execute_reply.started":"2024-08-26T06:31:32.286784Z"},"trusted":true},"outputs":[],"source":["final_metrics = []"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# loss_train = []\n","# loss_eval = []\n","# acc_train = []\n","# acc_eval = []"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T06:31:32.297217Z","iopub.status.busy":"2024-08-26T06:31:32.296923Z","iopub.status.idle":"2024-08-26T06:31:32.307991Z","shell.execute_reply":"2024-08-26T06:31:32.307024Z","shell.execute_reply.started":"2024-08-26T06:31:32.297192Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","import torch\n","import json\n","\n","file_path = 'metrics.json'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["checkpoint = torch.load('/kaggle/input/model3/pytorch/default/1/model-3.pt')\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with open('/kaggle/input/metrics/metrics.json', 'r') as json_file:\n","    final_metrics = json.load(json_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["file_path = '/kaggle/working/metrics.json'\n","with open(file_path, 'r') as json_file:\n","    final_metrics = json.load(json_file)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T06:31:32.310031Z","iopub.status.busy":"2024-08-26T06:31:32.309219Z","iopub.status.idle":"2024-08-26T06:31:32.320737Z","shell.execute_reply":"2024-08-26T06:31:32.319835Z","shell.execute_reply.started":"2024-08-26T06:31:32.309997Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["final_metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-26T06:31:32.322333Z","iopub.status.busy":"2024-08-26T06:31:32.322042Z"},"trusted":true},"outputs":[],"source":["for i in range(epoch):\n","    metrics = {}\n","    model.train()\n","    train_losses = 0.0\n","    train_sum = 0.0\n","    batch_iterator=tqdm(train_loader, desc=f\"Epoch {i+1:02d}\")\n","    for sample in batch_iterator:\n","        video, label = sample['video'], sample['label']\n","        \n","        video = video.to(device)\n","        label = torch.tensor([label], device=device)\n","\n","        optimizer.zero_grad() \n","        train_logits = model(video.permute(0, 2, 1, 3, 4))['logits']\n","        \n","        if train_logits.argmax() == label:\n","            train_sum +=1\n","                \n","        train_loss = F.cross_entropy(train_logits, label)\n","        train_losses += train_loss.item()\n","        \n","        \n","        train_loss.backward() \n","        optimizer.step() \n","    \n","    avg_train_loss = train_losses / train_dataset.num_videos\n","    train_acc = train_sum / train_dataset.num_videos\n","#     loss_train.append(float(avg_train_loss))\n","#     acc_train.append(float(train_acc))\n","    print(f'train loss: {avg_train_loss}')\n","    print(f'train acc: {train_acc}')\n","              \n","    model.eval()\n","    eval_losses = 0.0\n","    eval_sum = 0.0\n","    for sample in test_loader: \n","        video, label = sample['video'], sample['label']\n","        \n","        video = video.to(device)\n","        label = torch.tensor([label], device=device)\n","        \n","        with torch.no_grad():\n","            eval_logits = model(video.permute(0, 2, 1, 3, 4))['logits']\n","            eval_loss = F.cross_entropy(eval_logits, label).item()\n","            eval_losses += eval_loss\n","            \n","            if eval_logits.argmax() == label:\n","                eval_sum +=1\n","    with torch.no_grad():        \n","        avg_eval_loss = eval_losses / test_dataset.num_videos\n","        eval_acc = eval_sum / test_dataset.num_videos\n","    #     loss_eval.append(float(avg_eval_loss)) \n","    #     acc_eval.append(float(eval_acc))\n","        print(f'val loss: {avg_eval_loss}')\n","        print(f'val acc: {eval_acc}')\n","\n","        metrics['acc_train'] = train_acc\n","        metrics['loss_train'] = avg_train_loss\n","        metrics['acc_eval'] = eval_acc\n","        metrics['loss_eval'] = avg_eval_loss\n","        final_metrics.append(metrics)\n","\n","        torch.cuda.empty_cache()\n","\n","        PATH = f\"model-{i+1}.pt\"\n","        torch.save({\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                }, PATH)\n","\n","        with open(file_path, 'w') as json_file:\n","            json.dump(final_metrics, json_file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.save(model, 'model3.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["final_metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["label"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","os.remove(f'/kaggle/working/metrics.json')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["os.rename('/kaggle/working/model-2.pt', '/kaggle/working/model-1.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5572437,"sourceId":9215393,"sourceType":"datasetVersion"},{"datasetId":5581936,"sourceId":9229130,"sourceType":"datasetVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
