{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!pip install -q pytorchvideo evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pip install -q pyarrow==14.0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pip install -q transformers --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pip install -q torch==2.0.1 torchvision==0.15.2 --extra-index-url https://download.pytorch.org/whl/cu118 xformers==0.0.21"]},{"cell_type":"markdown","metadata":{},"source":["> # **Data Collection and Aggregation**"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T11:26:33.804129Z","iopub.status.busy":"2024-08-22T11:26:33.803767Z","iopub.status.idle":"2024-08-22T11:26:34.191372Z","shell.execute_reply":"2024-08-22T11:26:34.190401Z","shell.execute_reply.started":"2024-08-22T11:26:33.804095Z"},"trusted":true},"outputs":[{"data":{"text/plain":["label\n","normal      222\n","theft       156\n","fight       101\n","accident     97\n","Name: count, dtype: int64"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import pandas as pd\n","\n","root_path = \"/kaggle/input/my-data\"\n","folder_list = os.listdir(root_path)\n","label_list = [path for path in folder_list if not path.endswith((\".csv\"))]\n","total_df = pd.read_csv(os.path.join(root_path,\"train.csv\"))\n","\n","total_df.reset_index(drop = True, inplace = True)\n","total_df['label'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["> # **Data Splitting**"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T11:26:34.193098Z","iopub.status.busy":"2024-08-22T11:26:34.192732Z","iopub.status.idle":"2024-08-22T11:26:34.818031Z","shell.execute_reply":"2024-08-22T11:26:34.816865Z","shell.execute_reply.started":"2024-08-22T11:26:34.193065Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique classes: ['accident', 'fight', 'normal', 'theft'].\n","Splitted data: 460 116\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","def correct_file_path(file_name: str, root_path: str):\n","    return os.path.join(root_path, file_name.replace('\\\\', '/'))\n","\n","def preprocess_meta_df(df, root_path, label2id):\n","    df.rename(columns={\"video_name\": \"video_path\"}, inplace=True)\n","    df['video_path'] = df['video_path'].apply(lambda x: correct_file_path(x, root_path))\n","    df['label'] = df['label'].apply(lambda x: label2id[x])\n","    return df\n","\n","train_meta_df, test_meta_df = train_test_split(total_df, test_size=0.2, random_state=42)\n","\n","label_list = list(set(train_meta_df['label']))\n","class_labels = sorted(label_list)\n","label2id = {label: i for i, label in enumerate(class_labels)}\n","id2label = {i: label for label, i in label2id.items()}\n","\n","print(f\"Unique classes: {list(label2id.keys())}.\")\n","\n","train_meta_df = preprocess_meta_df(train_meta_df, root_path, label2id)\n","test_meta_df = preprocess_meta_df(test_meta_df, root_path, label2id)\n","\n","print(\"Splitted data:\", len(train_meta_df), len(test_meta_df))"]},{"cell_type":"markdown","metadata":{},"source":["> #  **Model Selection and Design**"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T11:26:34.819613Z","iopub.status.busy":"2024-08-22T11:26:34.819264Z","iopub.status.idle":"2024-08-22T11:26:49.320197Z","shell.execute_reply":"2024-08-22T11:26:49.319342Z","shell.execute_reply.started":"2024-08-22T11:26:34.819583Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-08-22 11:26:38.095150: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-08-22 11:26:38.095205: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-08-22 11:26:38.097230: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n","Some weights of VivitForVideoClassification were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([4]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of VivitForVideoClassification were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([4]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["import torch\n","import pytorchvideo.data\n","from torch.utils.data import Dataset\n","from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification, VivitConfig\n","from transformers import VivitImageProcessor, VivitModel, VivitForVideoClassification\n","import torch.nn as nn\n","\n","from pytorchvideo.transforms import (\n","    ApplyTransformToKey,\n","    Normalize,\n","    RandomShortSideScale,\n","    RemoveKey,\n","    ShortSideScale,\n","    UniformTemporalSubsample,\n",")\n","\n","from torchvision.transforms import (\n","    Compose,\n","    Lambda,\n","    RandomCrop,\n","    RandomHorizontalFlip,\n","    Resize,\n",")\n","\n","model_checkpoint = \"google/vivit-b-16x2-kinetics400\"\n","\n","model1 = VivitForVideoClassification.from_pretrained(model_checkpoint, label2id=label2id, id2label=id2label, ignore_mismatched_sizes=True)\n","\n","conf = model1.config\n","conf.hidden_dropout_prob = 0.1\n","conf.num_hidden_layers = 6\n","\n","model = VivitForVideoClassification.from_pretrained(model_checkpoint, config = conf, ignore_mismatched_sizes=True)\n","image_processor = VivitImageProcessor.from_pretrained(model_checkpoint)"]},{"cell_type":"markdown","metadata":{},"source":["> # **Data Preprocessing and Cleaning**"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T11:26:49.323060Z","iopub.status.busy":"2024-08-22T11:26:49.322436Z","iopub.status.idle":"2024-08-22T11:26:49.372565Z","shell.execute_reply":"2024-08-22T11:26:49.371787Z","shell.execute_reply.started":"2024-08-22T11:26:49.323031Z"},"trusted":true},"outputs":[],"source":["class CustomVideoDataset(Dataset):\n","    def __init__(self, dataframe):\n","        self.dataframe = dataframe\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        row = self.dataframe.iloc[idx]\n","        video_path = row['video_path']\n","        label = row['label']\n","        return video_path, label\n","\n","mean = image_processor.image_mean\n","std = image_processor.image_std\n","\n","if \"shortest_edge\" in image_processor.size:\n","    height = width = image_processor.size[\"shortest_edge\"]\n","else:\n","    height = image_processor.size[\"height\"]\n","    width = image_processor.size[\"width\"]\n","\n","resize_to = (model.config.image_size, model.config.image_size)\n","\n","num_frames_to_sample = model.config.num_frames\n","clip_duration = 8\n","\n","train_transform = Compose(\n","    [\n","        ApplyTransformToKey(\n","            key=\"video\",\n","            transform=Compose(\n","                [\n","                    UniformTemporalSubsample(num_frames_to_sample),\n","                    Lambda(lambda x: x / 255.0),\n","                    Normalize(mean, std),\n","                    RandomShortSideScale(min_size=256, max_size=320),\n","                    Resize(resize_to),\n","                    RandomHorizontalFlip(p=0.5),\n","                ]\n","            ),\n","        ),\n","    ]\n",")\n","\n","val_transform = Compose(\n","    [\n","        ApplyTransformToKey(\n","            key=\"video\",\n","            transform=Compose(\n","                [\n","                    UniformTemporalSubsample(num_frames_to_sample),\n","                    Lambda(lambda x: x / 255.0),\n","                    Normalize(mean, std),\n","                    Resize(resize_to),\n","                ]\n","            ),\n","        ),\n","    ]\n",")\n","\n","train_custom_dataset = CustomVideoDataset(train_meta_df)\n","train_labeled_video_paths = [(video_path, {'label': label}) for video_path, label in train_custom_dataset]\n","\n","test_custom_dataset = CustomVideoDataset(test_meta_df)\n","test_labeled_video_paths = [(video_path, {'label': label}) for video_path, label in test_custom_dataset]"]},{"cell_type":"markdown","metadata":{},"source":["> # **Data Loading**"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T11:26:49.374048Z","iopub.status.busy":"2024-08-22T11:26:49.373747Z","iopub.status.idle":"2024-08-22T11:26:49.392529Z","shell.execute_reply":"2024-08-22T11:26:49.391720Z","shell.execute_reply.started":"2024-08-22T11:26:49.374023Z"},"trusted":true},"outputs":[],"source":["import imageio\n","import numpy as np\n","from IPython.display import Image\n","\n","train_dataset = pytorchvideo.data.LabeledVideoDataset(\n","    labeled_video_paths =train_labeled_video_paths,\n","    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n","    decode_audio=False,\n","    transform=train_transform,\n",")\n","\n","test_dataset = pytorchvideo.data.LabeledVideoDataset(\n","    labeled_video_paths =test_labeled_video_paths,\n","    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n","    decode_audio=False,\n","    transform=val_transform,\n",")\n","\n","def unnormalize_img(img):\n","    img = (img * std) + mean\n","    img = (img * 255).astype(\"uint8\")\n","    return img.clip(0, 255)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T11:26:49.393938Z","iopub.status.busy":"2024-08-22T11:26:49.393625Z","iopub.status.idle":"2024-08-22T11:26:49.400440Z","shell.execute_reply":"2024-08-22T11:26:49.399353Z","shell.execute_reply.started":"2024-08-22T11:26:49.393909Z"},"trusted":true},"outputs":[],"source":["class CustomVideoDataset1(Dataset):\n","\n","    def __init__(self, data):\n","        super().__init__()\n","        self.train_dataset = data\n","\n","    def __len__(self):\n","        return self.train_dataset.num_videos\n","\n","    def __getitem__(self, idx):\n","        video=next(iter(self.train_dataset))\n","        \n","        return {\"video\": video['video'],\n","                \"label\": video['label']}"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T11:26:49.402141Z","iopub.status.busy":"2024-08-22T11:26:49.401813Z","iopub.status.idle":"2024-08-22T11:26:49.413208Z","shell.execute_reply":"2024-08-22T11:26:49.412357Z","shell.execute_reply.started":"2024-08-22T11:26:49.402117Z"},"trusted":true},"outputs":[],"source":["custom_train_data = CustomVideoDataset1(train_dataset)\n","custom_test_data = CustomVideoDataset1(test_dataset)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T11:26:49.414539Z","iopub.status.busy":"2024-08-22T11:26:49.414276Z","iopub.status.idle":"2024-08-22T11:26:49.427494Z","shell.execute_reply":"2024-08-22T11:26:49.426704Z","shell.execute_reply.started":"2024-08-22T11:26:49.414516Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T11:26:49.428642Z","iopub.status.busy":"2024-08-22T11:26:49.428349Z","iopub.status.idle":"2024-08-22T11:26:49.437484Z","shell.execute_reply":"2024-08-22T11:26:49.436543Z","shell.execute_reply.started":"2024-08-22T11:26:49.428619Z"},"trusted":true},"outputs":[],"source":["train_loader = DataLoader(custom_train_data, batch_size = 1)\n","test_loader = DataLoader(custom_test_data, batch_size = 1)"]},{"cell_type":"markdown","metadata":{},"source":["> # **Training**"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T11:26:49.438640Z","iopub.status.busy":"2024-08-22T11:26:49.438387Z","iopub.status.idle":"2024-08-22T11:26:49.452361Z","shell.execute_reply":"2024-08-22T11:26:49.451562Z","shell.execute_reply.started":"2024-08-22T11:26:49.438619Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import torch.optim as optim"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T11:26:49.453698Z","iopub.status.busy":"2024-08-22T11:26:49.453354Z","iopub.status.idle":"2024-08-22T11:26:49.643718Z","shell.execute_reply":"2024-08-22T11:26:49.642905Z","shell.execute_reply.started":"2024-08-22T11:26:49.453674Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","optimizer = optim.AdamW(model.parameters(), lr=5e-5)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T11:26:49.646723Z","iopub.status.busy":"2024-08-22T11:26:49.646418Z","iopub.status.idle":"2024-08-22T11:26:49.651216Z","shell.execute_reply":"2024-08-22T11:26:49.650118Z","shell.execute_reply.started":"2024-08-22T11:26:49.646697Z"},"trusted":true},"outputs":[],"source":["epoch = 5"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["final_metrics = []"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# loss_train = []\n","# loss_eval = []\n","# acc_train = []\n","# acc_eval = []"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T11:26:58.214943Z","iopub.status.busy":"2024-08-22T11:26:58.214207Z","iopub.status.idle":"2024-08-22T11:26:58.219624Z","shell.execute_reply":"2024-08-22T11:26:58.218538Z","shell.execute_reply.started":"2024-08-22T11:26:58.214900Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","import torch\n","import json\n","\n","file_path = 'metrics.json'"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T11:27:09.340889Z","iopub.status.busy":"2024-08-22T11:27:09.340473Z","iopub.status.idle":"2024-08-22T11:27:09.718771Z","shell.execute_reply":"2024-08-22T11:27:09.717853Z","shell.execute_reply.started":"2024-08-22T11:27:09.340843Z"},"trusted":true},"outputs":[],"source":["checkpoint = torch.load('/kaggle/working/model-8.pt')\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# with open('/kaggle/input/metrics/metrics.json', 'r') as json_file:\n","#     final_metrics = json.load(json_file)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T11:27:12.451808Z","iopub.status.busy":"2024-08-22T11:27:12.451388Z","iopub.status.idle":"2024-08-22T11:27:12.457728Z","shell.execute_reply":"2024-08-22T11:27:12.456623Z","shell.execute_reply.started":"2024-08-22T11:27:12.451773Z"},"trusted":true},"outputs":[],"source":["file_path = '/kaggle/working/metrics.json'\n","with open(file_path, 'r') as json_file:\n","    final_metrics = json.load(json_file)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T11:27:20.539285Z","iopub.status.busy":"2024-08-22T11:27:20.538863Z","iopub.status.idle":"2024-08-22T11:44:22.903318Z","shell.execute_reply":"2024-08-22T11:44:22.902199Z","shell.execute_reply.started":"2024-08-22T11:27:20.539252Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 01:   0%|          | 0/460 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","Epoch 01: 100%|██████████| 460/460 [13:50<00:00,  1.81s/it]\n"]},{"name":"stdout","output_type":"stream","text":["train loss: 0.15583108527757955\n","train acc: 0.9652173913043478\n","val loss: 0.9737459685129579\n","val acc: 0.6724137931034483\n"]}],"source":["for i in range(epoch):\n","    metrics = {}\n","    model.train()\n","    train_losses = 0.0\n","    train_sum = 0.0\n","    batch_iterator=tqdm(train_loader, desc=f\"Epoch {i+1:02d}\")\n","    for sample in batch_iterator:\n","        video, label = sample['video'], sample['label']\n","        \n","        video = video.to(device)\n","        label = torch.tensor([label], device=device)\n","\n","        optimizer.zero_grad() \n","        train_logits = model(video.permute(0, 2, 1, 3, 4))['logits']\n","        \n","        if train_logits.argmax() == label:\n","            train_sum +=1\n","                \n","        train_loss = F.cross_entropy(train_logits, label)\n","        train_losses += train_loss.item()\n","        \n","        \n","        train_loss.backward() \n","        optimizer.step() \n","    \n","    avg_train_loss = train_losses / train_dataset.num_videos\n","    train_acc = train_sum / train_dataset.num_videos\n","#     loss_train.append(float(avg_train_loss))\n","#     acc_train.append(float(train_acc))\n","    print(f'train loss: {avg_train_loss}')\n","    print(f'train acc: {train_acc}')\n","              \n","    model.eval()\n","    eval_losses = 0.0\n","    eval_sum = 0.0\n","    for sample in test_loader: \n","        video, label = sample['video'], sample['label']\n","        \n","        video = video.to(device)\n","        label = torch.tensor([label], device=device)\n","        \n","        with torch.no_grad():\n","            eval_logits = model(video.permute(0, 2, 1, 3, 4))['logits']\n","            eval_loss = F.cross_entropy(eval_logits, label).item()\n","            eval_losses += eval_loss\n","            \n","            if eval_logits.argmax() == label:\n","                eval_sum +=1\n","    with torch.no_grad():        \n","        avg_eval_loss = eval_losses / test_dataset.num_videos\n","        eval_acc = eval_sum / test_dataset.num_videos\n","    #     loss_eval.append(float(avg_eval_loss)) \n","    #     acc_eval.append(float(eval_acc))\n","        print(f'val loss: {avg_eval_loss}')\n","        print(f'val acc: {eval_acc}')\n","\n","        metrics['acc_train'] = train_acc\n","        metrics['loss_train'] = avg_train_loss\n","        metrics['acc_eval'] = eval_acc\n","        metrics['loss_eval'] = avg_eval_loss\n","        final_metrics.append(metrics)\n","\n","        torch.cuda.empty_cache()\n","\n","        PATH = f\"model-9.pt\"\n","        torch.save({\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                }, PATH)\n","\n","        with open(file_path, 'w') as json_file:\n","            json.dump(final_metrics, json_file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5572437,"sourceId":9215393,"sourceType":"datasetVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
